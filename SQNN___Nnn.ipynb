{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOs/STt1VzjTWYrsbrROHf/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Veranzi/Data_Science/blob/main/SQNN___Nnn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gym\n",
        "!pip install pygame\n",
        "!pip install tensorflow"
      ],
      "metadata": {
        "id": "Z8UIm4bQ9LTD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Importing libraries. The combination of these libraries suggests that the code may involve implementing and training a neural network for reinforcement learning within a specific environment (possibly a game-like environment using Pygame). The gym library provides the environment interface, and TensorFlow, along with Keras layers, is likely used for defining and training neural network models. Pygame is used for rendering and visualizing the environment during training or testing."
      ],
      "metadata": {
        "id": "iGGx1yLIbEbT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#libraries\n",
        "import numpy as np #used for numerical operations in Python, and in this context, it's likely used for array manipulations and mathematical computations.\n",
        "import gym  # Imports the OpenAI Gym library, a toolkit for developing and comparing reinforcement learning algorithms. It provides a variety of pre-built environments for testing and training agents.\n",
        "import pygame   #..Imports the Pygame library, a set of Python modules designed for writing video games. In this context, Pygame may be used for visualizing and rendering the reinforcement learning environment of the taxi.\n",
        "import tensorflow as tf#...used for building and training neural networks, including those used in reinforcement learning.\n",
        "from tensorflow.keras import layers #..Keras is a high-level neural networks API, and the layers module provides building blocks for constructing neural network architectures."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R3OQrXkMadmA",
        "outputId": "559b3a87-cd1a-4319-ab4c-562b057ba969"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/tensorflow/python/framework/dtypes.py:35: DeprecationWarning: ml_dtypes.float8_e4m3b11 is deprecated. Use ml_dtypes.float8_e4m3b11fnuz\n",
            "  from tensorflow.tsl.python.lib.core import pywrap_ml_dtypes\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code sets up a simple neural network architecture for a Q-network, which can be used in reinforcement learning algorithms to estimate Q-values and make decisions about actions in an environment.\n"
      ],
      "metadata": {
        "id": "0Zez2-AIaaQR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class QNetwork(tf.keras.Model): # The QNetwork class inherits from tf.keras.Model. It is designed for approximating Q-values for a given state in a reinforcement learning setting\n",
        "    def __init__(self, num_actions): #The constructor (init) initializes the Q-network with three dense (fully connected) layers.\n",
        "        super(QNetwork, self).__init__()\n",
        "\n",
        "        #The first two layers (dense1 and dense2) have 128 units each and use the ReLU (Rectified Linear Unit) activation function, which introduces non-linearity to the model.\n",
        "        self.dense1 = layers.Dense(128, activation='relu')\n",
        "        self.dense2 = layers.Dense(128, activation='relu')\n",
        "\n",
        "        #is the output layer with a number of units equal to the num_actions parameter, representing the possible actions in the reinforcement learning environment. It uses a linear activation function, as Q-values are unbounded.\n",
        "        self.output_layer = layers.Dense(num_actions, activation='linear')\n",
        "#The call method defines the forward pass of the network.\n",
        "#It takes a state as input and passes it through the two dense layers with ReLU activation functions, and then through the output layer with a linear activation function.\n",
        "#The result is the Q-values for each possible action in the given state.\n",
        "    def call(self, state):\n",
        "        x = self.dense1(state)\n",
        "        x = self.dense2(x)\n",
        "        return self.output_layer(x)"
      ],
      "metadata": {
        "id": "8WtVtBi5cDq-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code establishes a Gym environment for a taxi navigating a maze, sets up Q-networks for reinforcement learning, defines DQN-related parameters, and initializes a Pygame display for visualization.\n",
        "\n",
        "This code defines a custom environment TaxiDQNEnv for reinforcement learning using the OpenAI Gym framework. It models a taxi navigating a maze to pick up and drop off a passenger to the desired destination. The environment includes a Q-network for making decisions, target Q-network for stability, and various parameters for the Deep Q-Network (DQN) algorithm."
      ],
      "metadata": {
        "id": "2wfp0uCDcJ8F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TaxiDQNEnv(gym.Env):  #The TaxiDQNEnv class inherits from gym.Env, indicating it is an OpenAI Gym environment.\n",
        "    def __init__(self, taxi_maze): #The constructor (__init__) initializes the environment with a 2D maze represented by a NumPy array (taxi_maze).\n",
        "        super(TaxiDQNEnv, self).__init__()\n",
        "        self.taxi_maze = np.array(taxi_maze)\n",
        "        self.passenger_pos = np.where(self.taxi_maze == 'P') #The positions of the passenger ('P')\n",
        "        self.goal_pos = np.where(self.taxi_maze == 'D') # goal ('D')\n",
        "        self.taxi_pos = np.where(self.taxi_maze == 'T') # taxi ('T')\n",
        "        self.num_rows, self.num_cols = self.taxi_maze.shape\n",
        "#The observation space is defined as a tuple of discrete spaces representing the current positions of the taxi, passenger, and goal.\n",
        "        self.action_space = gym.spaces.Discrete(6)  # 6 possible actions: 0=up, 1=down, 2=left, 3=right, 4=pick up, 5=drop off\n",
        "        self.observation_space = gym.spaces.Tuple((\n",
        "            gym.spaces.Discrete(self.num_rows),\n",
        "            gym.spaces.Discrete(self.num_cols),\n",
        "            gym.spaces.Discrete(self.num_rows),\n",
        "            gym.spaces.Discrete(self.num_cols)\n",
        "        ))\n",
        " #Two instances of the QNetwork class (presumably defined elsewhere in the code) are created for representing Q-values.\n",
        " # One is the main Q-network (q_network), and the other is the target Q-network (target_q_network).\n",
        " #The weights of the target Q-network are initialized with the weights of the main Q-network.\n",
        "        # Neural network for Q-values\n",
        "        self.q_network = QNetwork(self.action_space.n)\n",
        "        self.target_q_network = QNetwork(self.action_space.n)\n",
        "        self.target_q_network.set_weights(self.q_network.get_weights())\n",
        "#DQN parameters are set, including the Adam optimizer with a learning rate of 0.001\n",
        "#a discount factor of 0.99 for future rewards, and a UCB (Upper Confidence Bound) exploration parameter (ucb_parameter) set to 1.0.\n",
        "        # DQN parameters\n",
        "        self.optimizer = tf.optimizers.Adam(learning_rate=0.001)\n",
        "        self.discount_factor = 0.99\n",
        "        self.ucb_parameter = 1.0  # UCB parameter for exploration\n",
        "#Pygame is initialized for visualizing the environment. The size of each cell in the visualization (cell_size) is set, and a Pygame display screen is created based on the dimensions of the maze.\n",
        "\n",
        "        pygame.init()\n",
        "        self.cell_size = 125\n",
        "        self.screen = pygame.display.set_mode((self.num_cols * self.cell_size, self.num_rows * self.cell_size))"
      ],
      "metadata": {
        "id": "FDNRUaNybQ0t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "returns a NumPy array representing the current state of the environment. The state includes the positions of the taxi and the passenger in the maze."
      ],
      "metadata": {
        "id": "5ZTklkA4jgm5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "    def _get_state(self):\n",
        "      #self.taxi_pos[0][0]: Retrieves the row index of the taxi's position in the maze.\n",
        "#self.taxi_pos[1][0]: Retrieves the column index of the taxi's position in the maze.\n",
        "#self.passenger_pos[0][0]: Retrieves the row index of the passenger's position in the maze.\n",
        "#self.passenger_pos[1][0]: Retrieves the column index of the passenger's position in the maze.\n",
        "        return np.array([self.taxi_pos[0][0], self.taxi_pos[1][0], self.passenger_pos[0][0], self.passenger_pos[1][0]])"
      ],
      "metadata": {
        "id": "KTQmEzwfjfM_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The step method in the TaxiDQNEnv class defines the logic for a single step in the reinforcement learning environment. It takes an action as input, updates the environment based on the action of the taxi, and performs Deep Q-Network (DQN) training."
      ],
      "metadata": {
        "id": "xbZ1acPJnCRj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "    def step(self, action):\n",
        "        state = self._get_state() #Retrieves the current state of the environment using the _get_state method.\n",
        "\n",
        "        # Take action and observe the next state and reward\n",
        "        next_state, reward, done, _ = self._take_action(action)\n",
        "\n",
        "        # DQN training\n",
        "        target = reward + self.discount_factor * np.max(self.target_q_network.predict(np.array([next_state]))) #Calculates the target Q-value for the Q-network update using the Bellman equation.computes the loss for the Q-network. It includes an Upper Confidence Bound (UCB) bonus to encourage exploration.\n",
        "        with tf.GradientTape() as tape: #Computes the gradients of the loss with respect to the trainable variables of the Q-network.\n",
        "            q_values = self.q_network(np.array([state]))\n",
        "            ucb_bonus = self.ucb_parameter * np.sqrt(np.log(self.total_steps + 1) / np.sum(self.total_actions + 1e-6))\n",
        "            action_ucb_values = q_values + ucb_bonus\n",
        "            loss = -tf.reduce_sum(tf.one_hot(action, self.action_space.n) * tf.math.log(tf.nn.softmax(action_ucb_values)))\n",
        "        gradients = tape.gradient(loss, self.q_network.trainable_variables)\n",
        "        self.optimizer.apply_gradients(zip(gradients, self.q_network.trainable_variables)) #Applies the gradients to update the Q-network's weights using the optimizer.\n",
        "\n",
        "        #  The target Q-network is updated periodically every 100 steps to improve stability.\n",
        "        if self.total_steps % 100 == 0:\n",
        "            self.target_q_network.set_weights(self.q_network.get_weights())\n",
        "\n",
        "        self.total_steps += 1 #Tracks the total number of steps taken in the environment.\n",
        "        self.total_actions[action] += 1  # Updates the count of actions taken for the Upper Confidence Bound (UCB) calculation\n",
        "\n",
        "        return next_state, reward, done, {} #The method returns the next state, the reward obtained, whether the episode is done, and an empty dictionary of additional information.\n"
      ],
      "metadata": {
        "id": "9b2r_r7lnA5r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This method encapsulates the transition dynamics of the environment based on the chosen action and is used within the step method to update the environment during the reinforcement learning process."
      ],
      "metadata": {
        "id": "oGt6np3Uvnt6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "    def _take_action(self, action):# defines the logic for taking a specific action in the reinforcement learning environment and updating the state, reward, and done status accordingly.\n",
        "        taxi_row, taxi_col, passenger_row, passenger_col = self._get_state() #The current state of the environment, including the positions of the taxi and the passenger, is obtained using the _get_state method.\n",
        "\n",
        "        reward = 0  # Initialize reward\n",
        "        done = False  # Initialize done\n",
        "\n",
        "        if action == 0:  # Up .........Decreases the taxi's row index, ensuring it doesn't go below the top boundary.\n",
        "            taxi_row = max(0, taxi_row - 1)\n",
        "        elif action == 1:  # Down............Increases the taxi's row index, ensuring it doesn't exceed the bottom boundary.\n",
        "            taxi_row = min(self.num_rows - 1, taxi_row + 1)\n",
        "        elif action == 2:  # Left.............Decreases the taxi's column index, ensuring it doesn't go below the left boundary.\n",
        "            taxi_col = max(0, taxi_col - 1)\n",
        "        elif action == 3:  # Right......... Increases the taxi's column index, ensuring it doesn't exceed the right boundary.\n",
        "            taxi_col = min(self.num_cols - 1, taxi_col + 1)\n",
        "        elif action == 4:  # Pick up.................If the taxi is in the same position as the passenger, the passenger is picked up, the corresponding cell in the maze is cleared, and the reward is set to 1.0.\n",
        "            if (taxi_row, taxi_col) == (passenger_row, passenger_col):\n",
        "                self.taxi_maze[passenger_row, passenger_col] = ' '\n",
        "                reward = 1.0  # Update reward for successful pick-up\n",
        "        elif action == 5:  # Drop off...................If the taxi is at the goal position and the passenger has been picked up, the drop-off is successful, and the reward is set to 1.0. If the drop-off is unsuccessful, the reward is set to -1.0.\n",
        "            if (taxi_row, taxi_col) == (self.goal_pos[0][0], self.goal_pos[1][0]) and \\\n",
        "                    self.taxi_maze[passenger_row, passenger_col] == ' ':\n",
        "                reward = 1.0  # Update reward for successful drop-off\n",
        "                done = True\n",
        "            else:\n",
        "                reward = -1.0  # Update reward for unsuccessful drop-off\n",
        "                done = False\n",
        "        else:\n",
        "            raise ValueError(\"Invalid action\")\n",
        "     #The method returns a tuple containing the next state, the reward obtained, the done status, and an empty dictionary of additional information.\n",
        "        return np.array([taxi_row, taxi_col, passenger_row, passenger_col]), reward, done, {}"
      ],
      "metadata": {
        "id": "2eWhsH9Wvhjn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "The reset method in the TaxiDQNEnv class is responsible for resetting the environment to its initial state at the beginning of a new episode."
      ],
      "metadata": {
        "id": "JIc75EchxZHk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "    def reset(self):\n",
        "        self.taxi_pos = np.where(self.taxi_maze == 'T')  #Resets the position of the taxi to its initial position by finding the coordinates where the character 'T' is located in the maze.\n",
        "        self.passenger_pos = np.where(self.taxi_maze == 'P')  #Resets the position of the passenger to its initial position by finding the coordinates where the character 'P' is located in the maze.\n",
        "        self.total_steps = 0 #Resets the total number of steps taken in the environment to zero for the new episode.\n",
        "        self.total_actions = np.zeros(self.action_space.n)  # Initializes an array of zeros to track the counts of each action taken. This is used for the Upper Confidence Bound (UCB) calculation.\n",
        "        return self._get_state() #Returns the initial state of the environment after the reset using the _get_state method. This state includes the positions of the taxi and the passenger."
      ],
      "metadata": {
        "id": "WV224d1nxMit"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "The render method in the TaxiDQNEnv class is responsible for visually rendering the current state of the environment using the Pygame library. It draws a representation of the maze on the screen, highlighting the positions of the taxi, passenger, and destination."
      ],
      "metadata": {
        "id": "PKOrnsL4ySKE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "    def render(self):\n",
        "        self.screen.fill((255, 255, 255)) #Fills the entire screen with a white color to clear the previous rendering.\n",
        "\n",
        "        for row in range(self.num_rows):\n",
        "            for col in range(self.num_cols):\n",
        "                cell_left = col * self.cell_size\n",
        "                cell_top = row * self.cell_size #Calculate the pixel coordinates of the top-left corner of the current cell based on its row and column.\n",
        "\n",
        "                if self.taxi_maze[row, col] == '#':  # Obstacle\n",
        "                    pygame.draw.rect(self.screen, (0, 0, 0), (cell_left, cell_top, self.cell_size, self.cell_size))\n",
        "                elif self.taxi_maze[row, col] == 'S':  # Starting position\n",
        "                    pygame.draw.rect(self.screen, (0, 255, 0), (cell_left, cell_top, self.cell_size, self.cell_size))\n",
        "                elif self.taxi_maze[row, col] == 'P':  # Passenger position\n",
        "                    pygame.draw.rect(self.screen, (255, 255, 0), (cell_left, cell_top, self.cell_size, self.cell_size))\n",
        "                elif self.taxi_maze[row, col] == 'D':  # Destination position\n",
        "                    pygame.draw.rect(self.screen, (255, 0, 0), (cell_left, cell_top, self.cell_size, self.cell_size))\n",
        "\n",
        "                if np.array_equal(self._get_state(), np.array([row, col, self.passenger_pos[0][0], self.passenger_pos[1][0]])): #draw a blue rectangle to highlight the taxi's position.\n",
        "                    pygame.draw.rect(self.screen, (0, 0, 255), (cell_left, cell_top, self.cell_size, self.cell_size)) #\n",
        "\n",
        "        pygame.display.update() #Updates the Pygame display to reflect the changes made in the rendering."
      ],
      "metadata": {
        "id": "Tew1hICSyQeE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Periodically, the total reward for the current episode is printed for monitoring the training progress."
      ],
      "metadata": {
        "id": "pAD8c06AzLeH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example Usage\n",
        "taxi_maze = [   #The layout of the taxi environment is defined as a 2D maze.\n",
        "    [' ', ' ', ' #', ' ', 'P'], #The total reward obtained in each episode is collected in the rewards list.\n",
        "    [' ', 'T', ' ', ' ', ' '],\n",
        "    [' ', '# ', ' ', ' #', ' '],\n",
        "    ['D', ' ', ' ', ' ', ' '],\n",
        "]\n",
        "\n",
        "env = TaxiDQNEnv(taxi_maze)\n",
        "\n",
        "# Training loop with rewards collection. After the training loop, you can analyze or visualize the training results using the rewards list.\n",
        "num_episodes = 1000 #number of episodes\n",
        "rewards = []"
      ],
      "metadata": {
        "id": "lOmRQHXbyFJF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The agent selects actions using UCB exploration. It calculates UCB values for each action based on the Q-values obtained from the neural network. The action with the highest UCB value is chosen."
      ],
      "metadata": {
        "id": "yltg6vfvz3tC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for episode in range(num_episodes): #The chosen action is used to take a step in the environment (env.step(action)), and the resulting state, reward, and done status are obtained.\n",
        "    state = env.reset()\n",
        "    total_reward = 0  #The total reward for the episode is updated.\n",
        "\n",
        "    while True:#The loop continues until the episode is done.\n",
        "        # Use UCB Exploration\n",
        "        q_values = env.q_network.predict(tf.convert_to_tensor([state]))[0]\n",
        "        q_values = env.q_network.predict(np.array([state]))[0]\n",
        "        q_values = np.asarray(q_values).astype(np.float32)\n",
        "        ucb_values = q_values + env.ucb_parameter * np.sqrt(np.log(env.total_steps + 1) / (env.total_actions + 1e-6))\n",
        "        action = np.argmax(ucb_values)\n",
        "#The training loop prints the episode number and the total reward for monitoring the training progress.\n",
        "        next_state, reward, done, _ = env.step(action)\n",
        "        total_reward += reward\n",
        "\n",
        "        state = next_state\n",
        "\n",
        "        if done:\n",
        "            break\n",
        "\n",
        "    rewards.append(total_reward)\n",
        "    print(f\"Episode: {episode + 1}, Total Reward: {total_reward}\")\n",
        "\n",
        "env.close() #After the training loop completes, the environment is closed."
      ],
      "metadata": {
        "id": "Zi9AL-jUz2Is"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The plot serves as a diagnostic tool to assess the performance and training progress of the agent"
      ],
      "metadata": {
        "id": "JqfagUTI1Fc4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9zY9dY3zaN0f"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Plotting the rewards\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.plot(rewards) #Plots the rewards list on the y-axis against the episode numbers on the x-axis. This line plot represents how the total reward changes over the course of training episodes.\n",
        "plt.title('Rewards Over Episodes') #Sets the title of the plot.\n",
        "plt.xlabel('Episode') #Sets the label for the x-axis.\n",
        "plt.ylabel('Total Reward') #Sets the label for the y-axis.\n",
        "plt.show() #Displays the plot\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "leuSRXUOaXgs"
      }
    }
  ]
}