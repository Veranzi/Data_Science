{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "19yileJtFjtpeg7mmNP1O8bBrle8F91TQ",
      "authorship_tag": "ABX9TyOYw8o+lLzo32SCFxFP+mAp",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Veranzi/Data_Science/blob/main/Copy_of_Copy_of_Fruits.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fnKRmjqM7MeA",
        "outputId": "edec7647-5788-4ed7-e49b-f6a3144da1b0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive/\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get set up\n",
        "import cv2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import sys # to access the system\n",
        "%matplotlib\n",
        "img = cv2.imread(\"/content/drive/MyDrive/fruits_dataset/images\")\n"
      ],
      "metadata": {
        "id": "0xsYL3vaU_Bz",
        "outputId": "36fe06c3-8ff6-4b11-a04a-cb17601c42f3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using matplotlib backend: <object object at 0x7f9897e3b470>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "FdPLVKxhW8qP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Data Augmentation\n",
        "datagen = ImageDataGenerator(rotation_range=10,\n",
        "            rescale = 1./255,\n",
        "            width_shift_range=0.1,\n",
        "            height_shift_range=0.1,\n",
        "            horizontal_flip=True,\n",
        "            vertical_flip=False,\n",
        "            zoom_range=0.1,\n",
        "            shear_range=0.1,\n",
        "            brightness_range=[0.8, 1.2],\n",
        "            fill_mode='nearest',\n",
        "            validation_split=0.2  # set validation split to 20%\n",
        "            )"
      ],
      "metadata": {
        "id": "EGEDM8M-DSDy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_set = datagen.flow_from_directory(\"/content/drive/MyDrive/fruits_dataset/images\",\n",
        "                                              batch_size = 32,\n",
        "                                              class_mode = 'categorical',\n",
        "                                              target_size=(64,64),\n",
        "                                              subset = 'training'\n",
        "                                            )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Crd2RJMDYsa",
        "outputId": "7ea1bf82-55e2-4bc8-d84b-5feb3fc27d04"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 288 images belonging to 9 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_set = datagen.flow_from_directory(\"/content/drive/MyDrive/fruits_dataset/images\",\n",
        "                                              batch_size = 32,\n",
        "                                              class_mode = 'categorical',\n",
        "                                              target_size=(64,64),\n",
        "                                              subset = 'validation'\n",
        "                                            )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7QZlVFFSTM6w",
        "outputId": "5b33ca7f-591a-4020-d640-5510e8a76802"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 71 images belonging to 9 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "training_set.classes"
      ],
      "metadata": {
        "id": "k2T-bdwhXqoe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "317addb1-47e2-4c13-81d2-c5c212747bcd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2,\n",
              "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
              "       2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
              "       3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 4, 4, 4, 4,\n",
              "       4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
              "       4, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5,\n",
              "       5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 6, 6, 6, 6, 6, 6,\n",
              "       6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6,\n",
              "       6, 6, 6, 6, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7,\n",
              "       7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 8, 8, 8, 8, 8, 8, 8, 8,\n",
              "       8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,\n",
              "       8, 8], dtype=int32)"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "training_set.class_indices"
      ],
      "metadata": {
        "id": "l40EKOD6Xwb_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "273a14a1-ba1e-4b7d-d725-889aee2f32d5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'apple fruit': 0,\n",
              " 'banana fruit': 1,\n",
              " 'cherry fruit': 2,\n",
              " 'chickoo fruit': 3,\n",
              " 'grapes fruit': 4,\n",
              " 'kiwi fruit': 5,\n",
              " 'mango fruit': 6,\n",
              " 'orange fruit': 7,\n",
              " 'strawberry fruit': 8}"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "CNN NETWORK"
      ],
      "metadata": {
        "id": "z1kIfMfNX5-_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initializing the CNN\n",
        "\n",
        "cnn = tf.keras.models.Sequential()"
      ],
      "metadata": {
        "id": "gHYIUPv7X8sB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convolution\n",
        "cnn.add(tf.keras.layers.Conv2D(filters=32, kernel_size=3, activation='relu', input_shape=[64, 64, 3]))"
      ],
      "metadata": {
        "id": "_QBwOfLJYBhi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Pooling\n",
        "cnn.add(tf.keras.layers.MaxPool2D(pool_size=2, strides=2))"
      ],
      "metadata": {
        "id": "DvcCC6gsYEki"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Adding a second convolutional layer\n",
        "cnn.add(tf.keras.layers.Conv2D(filters=32, kernel_size=3, activation='relu'))\n",
        "cnn.add(tf.keras.layers.MaxPool2D(pool_size=2, strides=2))"
      ],
      "metadata": {
        "id": "hENTE7UAYKDR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Flattening\n",
        "cnn.add(tf.keras.layers.Flatten())"
      ],
      "metadata": {
        "id": "ogN5o7fRYOix"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Full Connection\n",
        "cnn.add(tf.keras.layers.Dense(units=128, activation='relu'))"
      ],
      "metadata": {
        "id": "drBaqKUOYVj8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Output Layer\n",
        "cnn.add(tf.keras.layers.Dense(units=9, activation='sigmoid'))"
      ],
      "metadata": {
        "id": "2HnSDJgBYbXt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "TRAINING"
      ],
      "metadata": {
        "id": "xZCsJNlOYfuO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Compiling the CNN\n",
        "cnn.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics = ['accuracy'])"
      ],
      "metadata": {
        "id": "T8659KbHYhb1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training the CNN on the Training set and evaluating it on the Test set\n",
        "cnn.fit(x = training_set, validation_data = test_set, epochs = 40)"
      ],
      "metadata": {
        "id": "0QHWBvlTYjtb",
        "outputId": "02577eba-6ab4-4d45-dcd7-26b7559bd861",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/40\n",
            "9/9 [==============================] - 153s 17s/step - loss: 2.2263 - accuracy: 0.1910 - val_loss: 2.0460 - val_accuracy: 0.3099\n",
            "Epoch 2/40\n",
            "9/9 [==============================] - 25s 3s/step - loss: 1.9108 - accuracy: 0.3368 - val_loss: 1.6796 - val_accuracy: 0.4225\n",
            "Epoch 3/40\n",
            "9/9 [==============================] - 26s 3s/step - loss: 1.6274 - accuracy: 0.4236 - val_loss: 1.5132 - val_accuracy: 0.3239\n",
            "Epoch 4/40\n",
            "9/9 [==============================] - 23s 3s/step - loss: 1.5370 - accuracy: 0.4375 - val_loss: 1.4106 - val_accuracy: 0.4789\n",
            "Epoch 5/40\n",
            "9/9 [==============================] - 26s 3s/step - loss: 1.4195 - accuracy: 0.4583 - val_loss: 1.2852 - val_accuracy: 0.4789\n",
            "Epoch 6/40\n",
            "9/9 [==============================] - 26s 3s/step - loss: 1.2903 - accuracy: 0.5208 - val_loss: 1.2403 - val_accuracy: 0.5493\n",
            "Epoch 7/40\n",
            "9/9 [==============================] - 24s 3s/step - loss: 1.2437 - accuracy: 0.5729 - val_loss: 1.3007 - val_accuracy: 0.5493\n",
            "Epoch 8/40\n",
            "9/9 [==============================] - 25s 3s/step - loss: 1.1993 - accuracy: 0.5694 - val_loss: 1.2391 - val_accuracy: 0.5352\n",
            "Epoch 9/40\n",
            "9/9 [==============================] - 25s 3s/step - loss: 1.1637 - accuracy: 0.5799 - val_loss: 1.2038 - val_accuracy: 0.5775\n",
            "Epoch 10/40\n",
            "9/9 [==============================] - 24s 3s/step - loss: 1.0952 - accuracy: 0.6146 - val_loss: 1.2111 - val_accuracy: 0.5915\n",
            "Epoch 11/40\n",
            "9/9 [==============================] - 22s 2s/step - loss: 1.0754 - accuracy: 0.6076 - val_loss: 1.1406 - val_accuracy: 0.5775\n",
            "Epoch 12/40\n",
            "9/9 [==============================] - 24s 3s/step - loss: 1.0751 - accuracy: 0.6354 - val_loss: 1.1277 - val_accuracy: 0.5634\n",
            "Epoch 13/40\n",
            "9/9 [==============================] - 25s 3s/step - loss: 1.0181 - accuracy: 0.6285 - val_loss: 0.9632 - val_accuracy: 0.6479\n",
            "Epoch 14/40\n",
            "9/9 [==============================] - 24s 3s/step - loss: 0.9462 - accuracy: 0.6771 - val_loss: 1.0804 - val_accuracy: 0.5775\n",
            "Epoch 15/40\n",
            "9/9 [==============================] - 25s 3s/step - loss: 0.8903 - accuracy: 0.7083 - val_loss: 1.0260 - val_accuracy: 0.6338\n",
            "Epoch 16/40\n",
            "9/9 [==============================] - 24s 3s/step - loss: 0.9240 - accuracy: 0.6875 - val_loss: 0.9752 - val_accuracy: 0.6056\n",
            "Epoch 17/40\n",
            "9/9 [==============================] - 25s 3s/step - loss: 0.8802 - accuracy: 0.6736 - val_loss: 1.0409 - val_accuracy: 0.5634\n",
            "Epoch 18/40\n",
            "9/9 [==============================] - 23s 3s/step - loss: 0.8457 - accuracy: 0.7118 - val_loss: 0.9392 - val_accuracy: 0.7042\n",
            "Epoch 19/40\n",
            "9/9 [==============================] - 24s 3s/step - loss: 0.8113 - accuracy: 0.7049 - val_loss: 0.8485 - val_accuracy: 0.7042\n",
            "Epoch 20/40\n",
            "9/9 [==============================] - 24s 3s/step - loss: 0.7453 - accuracy: 0.7361 - val_loss: 1.0330 - val_accuracy: 0.6479\n",
            "Epoch 21/40\n",
            "9/9 [==============================] - 24s 3s/step - loss: 0.7053 - accuracy: 0.7778 - val_loss: 0.9982 - val_accuracy: 0.6338\n",
            "Epoch 22/40\n",
            "9/9 [==============================] - 26s 3s/step - loss: 0.5892 - accuracy: 0.8229 - val_loss: 0.8062 - val_accuracy: 0.7183\n",
            "Epoch 23/40\n",
            "9/9 [==============================] - 24s 3s/step - loss: 0.5613 - accuracy: 0.8264 - val_loss: 0.8381 - val_accuracy: 0.6620\n",
            "Epoch 24/40\n",
            "9/9 [==============================] - 25s 3s/step - loss: 0.5907 - accuracy: 0.7778 - val_loss: 0.9660 - val_accuracy: 0.6901\n",
            "Epoch 25/40\n",
            "9/9 [==============================] - 24s 3s/step - loss: 0.6045 - accuracy: 0.7847 - val_loss: 0.9148 - val_accuracy: 0.6338\n",
            "Epoch 26/40\n",
            "9/9 [==============================] - 24s 3s/step - loss: 0.5483 - accuracy: 0.8264 - val_loss: 1.0660 - val_accuracy: 0.6901\n",
            "Epoch 27/40\n",
            "9/9 [==============================] - 23s 3s/step - loss: 0.4909 - accuracy: 0.8368 - val_loss: 0.8950 - val_accuracy: 0.7042\n",
            "Epoch 28/40\n",
            "9/9 [==============================] - 24s 3s/step - loss: 0.5096 - accuracy: 0.8229 - val_loss: 0.9619 - val_accuracy: 0.7042\n",
            "Epoch 29/40\n",
            "9/9 [==============================] - 24s 3s/step - loss: 0.4716 - accuracy: 0.8472 - val_loss: 1.0655 - val_accuracy: 0.6479\n",
            "Epoch 30/40\n",
            "9/9 [==============================] - 30s 3s/step - loss: 0.5155 - accuracy: 0.8264 - val_loss: 1.1480 - val_accuracy: 0.7042\n",
            "Epoch 31/40\n",
            "9/9 [==============================] - 25s 3s/step - loss: 0.5196 - accuracy: 0.8090 - val_loss: 0.9344 - val_accuracy: 0.6901\n",
            "Epoch 32/40\n",
            "9/9 [==============================] - 22s 3s/step - loss: 0.3967 - accuracy: 0.8646 - val_loss: 0.9384 - val_accuracy: 0.7324\n",
            "Epoch 33/40\n",
            "9/9 [==============================] - 24s 3s/step - loss: 0.4214 - accuracy: 0.8576 - val_loss: 1.0161 - val_accuracy: 0.7042\n",
            "Epoch 34/40\n",
            "9/9 [==============================] - 24s 3s/step - loss: 0.4434 - accuracy: 0.8576 - val_loss: 0.8455 - val_accuracy: 0.7183\n",
            "Epoch 35/40\n",
            "9/9 [==============================] - 29s 3s/step - loss: 0.4315 - accuracy: 0.8299 - val_loss: 0.9891 - val_accuracy: 0.6901\n",
            "Epoch 36/40\n",
            "9/9 [==============================] - 29s 3s/step - loss: 0.3727 - accuracy: 0.8750 - val_loss: 1.0087 - val_accuracy: 0.6761\n",
            "Epoch 37/40\n",
            "9/9 [==============================] - 26s 3s/step - loss: 0.4039 - accuracy: 0.8403 - val_loss: 1.1272 - val_accuracy: 0.6761\n",
            "Epoch 38/40\n",
            "9/9 [==============================] - 24s 3s/step - loss: 0.3728 - accuracy: 0.8576 - val_loss: 1.0520 - val_accuracy: 0.6620\n",
            "Epoch 39/40\n",
            "9/9 [==============================] - 29s 3s/step - loss: 0.3569 - accuracy: 0.8715 - val_loss: 1.0137 - val_accuracy: 0.6620\n",
            "Epoch 40/40\n",
            "9/9 [==============================] - 25s 3s/step - loss: 0.2974 - accuracy: 0.9201 - val_loss: 0.9659 - val_accuracy: 0.7746\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f97e8280a30>"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "PREDICTION ON A SINGLE IMAGE"
      ],
      "metadata": {
        "id": "f3Dim13DY2dr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing import image\n",
        "test_image = image.load_img('/content/drive/MyDrive/fruits_dataset/images/strawberry fruit/Image_29.jpg', target_size = (64, 64))\n",
        "test_image = image.img_to_array(test_image)\n",
        "test_image = np.expand_dims(test_image, axis = 0)\n",
        "test_image /= 255\n",
        "result = cnn.predict(test_image)\n",
        "sample = np.argmax(result)\n",
        "sample\n",
        "\n"
      ],
      "metadata": {
        "id": "CbPVtNfSY6Cc",
        "outputId": "107ba570-c1cb-482a-a068-ced76c077d1e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 155ms/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "8"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "Pkl_Filename = \"modelFruit.pkl\"\n",
        "\n",
        "with open(Pkl_Filename, 'wb') as file:\n",
        "    pickle.dump(result, file)"
      ],
      "metadata": {
        "id": "qRKM10UUi-R7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "{'apple fruit': 0,\n",
        " 'banana fruit': 1,\n",
        " 'cherry fruit': 2,\n",
        " 'chickoo fruit': 3,\n",
        " 'grapes fruit': 4,\n",
        " 'kiwi fruit': 5,\n",
        " 'mango fruit': 6,\n",
        " 'orange fruit': 7,\n",
        " 'strawberry fruit': 8}"
      ],
      "metadata": {
        "id": "i7eY5W9yZWvb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow import keras"
      ],
      "metadata": {
        "id": "XK9ux4HTkUEq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_model():\n",
        "    # Create a simple model.\n",
        "    inputs = keras.Input(shape=(32,))\n",
        "    outputs = keras.layers.Dense(1)(inputs)\n",
        "    model = keras.Model(inputs, outputs)\n",
        "    model.compile(optimizer=\"adam\", loss=\"mean_squared_error\")\n",
        "    return model\n"
      ],
      "metadata": {
        "id": "DzQEgtFXjrq6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = get_model()"
      ],
      "metadata": {
        "id": "uqgu4Oa4lQci"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.save(\"my_model\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rF_2o6vVkyB5",
        "outputId": "67f73f65-34c9-4728-f8c2-2d6a94641a4b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as _update_step_xla while saving (showing 1 of 1). These functions will not be directly callable after loading.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# Assuming you have already trained and obtained a TensorFlow model\n",
        "model = ...\n",
        "\n",
        "# Define the export path where the saved model will be stored\n",
        "export_path = '/content/drive/MyDrive/fruits_dataset/'\n",
        "\n",
        "# Create a TensorFlow SavedModelBuilder object\n",
        "builder = tf.saved_model.builder.SavedModelBuilder(export_path)\n",
        "\n",
        "# Create a signature for the model's input and output tensors\n",
        "input_tensor = model.input  # Replace with your model's input tensor\n",
        "output_tensor = model.output  # Replace with your model's output tensor\n",
        "\n",
        "# Create a TensorFlow serving signature definition\n",
        "signature = tf.saved_model.signature_def_utils.predict_signature_def(\n",
        "    inputs={'input': input_tensor},\n",
        "    outputs={'output': output_tensor}\n",
        ")\n",
        "\n",
        "# Add the signature definition to the SavedModelBuilder\n",
        "builder.add_meta_graph_and_variables(\n",
        "    sess=tf.keras.backend.get_session(),  # Assuming you are using Keras\n",
        "    tags=[tf.saved_model.SERVING],\n",
        "    signature_def_map={'predict': signature}\n",
        ")\n",
        "\n",
        "# Save the model\n",
        "builder.save()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "id": "LL03otRlTqAJ",
        "outputId": "73ff3df9-83b2-4b6e-f350-6bd2067ad089"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-22-f7238bdfeeae>\u001b[0m in \u001b[0;36m<cell line: 10>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# Create a TensorFlow SavedModelBuilder object\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mbuilder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msaved_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuilder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSavedModelBuilder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexport_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m# Create a signature for the model's input and output tensors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: module 'tensorflow._api.v2.saved_model' has no attribute 'builder'"
          ]
        }
      ]
    }
  ]
}